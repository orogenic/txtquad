#pragma once

#include <float.h>
#include <stdarg.h>
#include <stdbool.h>
#define generic _Generic // lol no <stdgeneric.h>
#include <stdio.h>

#ifndef _WIN32
# include <tgmath.h>
#else
# include <math.h> // Windows doesn't support _Complex, Clang won't compile tgmath.h correctly.
                   // Using double variants for everything seems to be fine... who knows
#endif

#include "types.h"

typedef struct { v3 t; v4 r; } tqtr;

typedef struct {
	union {
		tqtr tr;
		struct { v3 t; v4 r; };
	};
	f32 s;
} tqtrs;

#ifdef AlgInline
# define inline __attribute__((__always_inline__))
#endif

#ifdef AlgIntrin
# include "algintrin.h"
# define intrin(...)
// could runtime CPUID detection could set function pointers for intrinsics/fallbacks?
#else
# define intrin(...) static __VA_ARGS__
#endif

#ifdef AlgAssert
# include <assert.h>
# define algassert assert
#else
# define algassert(...)
#endif

#include "swizzles.h" // generated by swizzles.exe

#define AlgE     2.71828182845904523536
#define AlgPi    3.14159265358979323846
#define AlgSqrt2 1.41421356237309504880

#define V2(x, y)       ((v2) { x, y })
#define V3(x, y, z)    ((v3) { x, y, z })
#define V4(x, y, z, w) ((v4) { x, y, z, w })

#define M2( c0r0, c0r1 \
          , c1r0, c1r1 ) ((m2) { c0r0, c0r1 \
                               , c1r0, c1r1 })
#define M3( c0r0, c0r1, c0r2 \
          , c1r0, c1r1, c1r2 \
          , c2r0, c2r1, c2r2 ) ((m3) { c0r0, c0r1, c0r2 \
                                     , c1r0, c1r1, c1r2 \
                                     , c2r0, c2r1, c2r2 })
#define M4( c0r0, c0r1, c0r2, c0r3 \
          , c1r0, c1r1, c1r2, c1r3 \
          , c2r0, c2r1, c2r2, c2r3 \
          , c3r0, c3r1, c3r2, c3r3 ) ((m4) { c0r0, c0r1, c0r2, c0r3 \
                                           , c1r0, c1r1, c1r2, c1r3 \
                                           , c2r0, c2r1, c2r2, c2r3 \
                                           , c3r0, c3r1, c3r2, c3r3 })

#define M2c(c0, c1)         ((m2) { .c = { c0, c1 } })
#define M3c(c0, c1, c2)     ((m3) { .c = { c0, c1, c2 } })
#define M4c(c0, c1, c2, c3) ((m4) { .c = { c0, c1, c2, c3 } })

#define M2c0(C0) ((m2) { .c0 = C0 })
#define M3c0(C0) ((m3) { .c0 = C0 })
#define M4c0(C0) ((m4) { .c0 = C0 })
#define M2c1(C1) ((m2) { .c1 = C1 })
#define M3c1(C1) ((m3) { .c1 = C1 })
#define M4c1(C1) ((m4) { .c1 = C1 })
#define M3c2(C2) ((m3) { .c2 = C2 })
#define M4c2(C2) ((m4) { .c2 = C2 })
#define M4c3(C3) ((m4) { .c3 = C3 })

#define Vx(X)             { .x = X }
#define Vy(Y)             { .y = Y }
#define Vz(Z)             { .z = Z }
#define Vw(W)             { .w = W }
#define Vxy(X, Y)         { .x = X, .y = Y }
#define Vxz(X, Z)         { .x = X, .z = Z }
#define Vxw(X, W)         { .x = X, .w = W }
#define Vyz(Y, Z)         { .y = Y, .z = Z }
#define Vyw(Y, W)         { .y = Y, .w = W }
#define Vzw(Z, W)         { .z = Z, .w = W }
#define Vxyz(X, Y, Z)     { .x = X, .y = Y, .z = Z }
#define Vxyw(X, Y, W)     { .x = X, .y = Y, .w = W }
#define Vxzw(X, Z, W)     { .x = X, .z = Z, .w = W }
#define Vyzw(Y, Z, W)     { .y = Y, .z = Z, .w = W }
#define Vxyzw(X, Y, Z, W) { .x = X, .y = W, .z = Z, .w = W }

#define V2x(x)             ((v2) Vx(x))
#define V3x(x)             ((v3) Vx(x))
#define V4x(x)             ((v4) Vx(x))
#define V2y(y)             ((v2) Vy(y))
#define V3y(y)             ((v3) Vy(y))
#define V4y(y)             ((v4) Vy(y))
#define V3z(z)             ((v3) Vz(z))
#define V4z(z)             ((v4) Vz(z))
#define V4w(w)             ((v4) Vw(w))
#define V2xy(x, y)         ((v2) Vxy(x, y))
#define V3xy(x, y)         ((v3) Vxy(x, y))
#define V4xy(x, y)         ((v4) Vxy(x, y))
#define V3xz(x, z)         ((v3) Vxz(x, z))
#define V4xz(x, z)         ((v4) Vxz(x, z))
#define V4xw(x, w)         ((v4) Vxw(x, w))
#define V3yz(y, z)         ((v3) Vyz(y, z))
#define V4yz(y, z)         ((v4) Vyz(y, z))
#define V4yw(y, w)         ((v4) Vyw(y, w))
#define V4zw(z, w)         ((v4) Vzw(z, w))
#define V3xyz(x, y, z)     ((v3) Vxyz(x, y, z))
#define V4xyz(x, y, z)     ((v4) Vxyz(x, y, z))
#define V4xyw(x, y, w)     ((v4) Vxyw(x, y, w))
#define V4xzw(x, z, w)     ((v4) Vxzw(x, z, w))
#define V4yzw(y, z, w)     ((v4) Vyzw(y, z, w))
#define V4xyzw(x, y, z, w) ((v4) Vxyzw(x, y, z, w))

#define V2e(e) ((v2) { e, e })
#define V3e(e) ((v3) { e, e, e })
#define V4e(e) ((v4) { e, e, e, e })

#define M2e(e) ((m2) { e, e \
                     , e, e })
#define M3e(e) ((m3) { e, e, e \
                     , e, e, e \
                     , e, e, e })
#define M4e(e) ((m4) { e, e, e, e \
                     , e, e, e, e \
                     , e, e, e, e \
                     , e, e, e, e })

#define V3ex(e, x)     ((v3) { x, e, e })
#define V3ey(e, y)     ((v3) { e, y, e })
#define V3ez(e, z)     ((v3) { e, e, z })
#define V4ex(e, x)     ((v4) { x, e, e, e })
#define V4ey(e, y)     ((v4) { e, y, e, e })
#define V4ez(e, z)     ((v4) { e, e, z, e })
#define V4ew(e, w)     ((v4) { e, e, e, w })
#define V4exy(e, x, y) ((v4) { x, y, e, e })
#define V4exz(e, x, z) ((v4) { x, e, z, e })
#define V4exw(e, x, w) ((v4) { x, e, e, w })
#define V4eyz(e, y, z) ((v4) { e, y, z, e })
#define V4eyw(e, y, w) ((v4) { e, y, e, w })
#define V4ezw(e, z, w) ((v4) { e, e, z, w })

#define M2diag(a,b)     ((m2) { .c0.r0 = a \
                              , .c1.r1 = b })
#define M3diag(a,b,c)   ((m3) { .c0.r0 = a \
                              , .c1.r1 = b \
                              , .c2.r2 = c })
#define M4diag(a,b,c,d) ((m4) { .c0.r0 = a \
                              , .c1.r1 = b \
                              , .c2.r2 = c \
                              , .c3.r3 = d })

#define V20 ((v2) { 0 })
#define V30 ((v3) { 0 })
#define V40 ((v4) { 0 })
#define M20 ((m2) { 0 })
#define M30 ((m3) { 0 })
#define M40 ((m4) { 0 })

#define V21 V2e(1.f)
#define V31 V3e(1.f)
#define V41 V4e(1.f)
#define M21 M2e(1.f)
#define M31 M3e(1.f)
#define M41 M4e(1.f)

#define M2i M2diag(1.f, 1.f)
#define M3i M3diag(1.f, 1.f, 1.f)
#define M4i M4diag(1.f, 1.f, 1.f, 1.f)

#define V2smul(s, x, y)       ((v2) { s * x, s * y })
#define V3smul(s, x, y, z)    ((v3) { s * x, s * y, s * z })
#define V4smul(s, x, y, z, w) ((v4) { s * x, s * y, s * z, s * w })

#define M2smul(s, c0r0, c0r1 \
                , c1r0, c1r1 ) ((m2) { s * c0r0, s * c0r1 \
                                     , s * c1r0, s * c1r1 })
#define M3smul(s, c0r0, c0r1, c0r2 \
                , c1r0, c1r1, c1r2 \
                , c2r0, c2r1, c2r2 ) ((m3) { s * c0r0, s * c0r1, s * c0r2 \
                                           , s * c1r0, s * c1r1, s * c1r2 \
                                           , s * c2r0, s * c2r1, s * c2r2 })
#define M4smul(s, c0r0, c0r1, c0r2, c0r3 \
                , c1r0, c1r1, c1r2, c1r3 \
                , c2r0, c2r1, c2r2, c2r3 \
                , c3r0, c3r1, c3r2, c3r3 ) ((m4) { s * c0r0, s * c0r1, s * c0r2, s * c0r3 \
                                                 , s * c1r0, s * c1r1, s * c1r2, s * c1r3 \
                                                 , s * c2r0, s * c2r1, s * c2r2, s * c2r3 \
                                                 , s * c3r0, s * c3r1, s * c3r2, s * c3r3 })

#define V2rt  V2x( 1.f)
#define V3rt  V3x( 1.f)
#define V4rt  V4x( 1.f)
#define V2lft V2x(-1.f)
#define V3lft V3x(-1.f)
#define V4lft V4x(-1.f)
#define V2up  V2y( 1.f)
#define V3up  V3y( 1.f)
#define V4up  V4y( 1.f)
#define V2dwn V2y(-1.f)
#define V3dwn V3y(-1.f)
#define V4dwn V4y(-1.f)
#define V3fwd V3z( 1.f)
#define V4fwd V4z( 1.f)
#define V3bck V3z(-1.f)
#define V4bck V4z(-1.f)

#define Qi   V4w(1.f)
#define Qrt  Qi
#define Qlft V4z(1.f)
#define Qup  V4zw( AlgSqrt2 * .5f, AlgSqrt2 * .5f)
#define Qdwn V4zw(-AlgSqrt2 * .5f, AlgSqrt2 * .5f)

#define   fop(op) f32 : op      , f64 : op
#define   Fop(op) f32 : op##f32 , f64 : op##f64
#define  pFop(op) f32*: op##f32p, f64*: op##f64p
#define  PFop(op) Fop(op), pFop(op)

#define   Vop(op) v2  : op##v2  , v3  : op##v3  , v4  : op##v4
#define  pVop(op) v2 *: op##v2p , v3 *: op##v3p , v4 *: op##v4p
#define  PVop(op) Vop(op), pVop(op)

#define   Mop(op) m2  : op##m2  , m3  : op##m3  , m4  : op##m4
#define  pMop(op) m2 *: op##m2p , m3 *: op##m3p , m4 *: op##m4p
#define  PMop(op) Mop(op), pMop(op)

#define  VMop(op) Vop(op), Mop(op)
#define FVMop(op) Vop(op), Mop(op), Fop(op)

#define PVMop(op) PVop(op), PMop(op)

// 0 or 1 varargs
#define v4v1(a, ...) generic(a, fop(v4v112), v2: v4v121, v3: v4v13)
#define v4v2(a, ...) generic(a, fop(v4v211), v2: v4v22)

// 1 or 2 varargs
#define v4v(a, ...) generic(a, fop(v4v1(__VA_ARGS__)), v2: v4v2(__VA_ARGS__), v3: v4v31) (a, __VA_ARGS__)
static inline v4 v4v112 (f32 const x , f32 const y , v2  const zw) { return V4(x, y, zw.x, zw.y); }
static inline v4 v4v121 (f32 const x , v2  const yz, f32 const w ) { return V4(x, yz.x, yz.y, w); }
static inline v4 v4v211 (v2  const xy, f32 const z , f32 const w ) { return V4(xy.x, xy.y, z, w); }
static inline v4 v4v22  (v2  const xy , v2  const zw ) { return (v4) { .xy  = xy , .zw  = zw  }; }
static inline v4 v4v13  (f32 const x  , v3  const yzw) { return (v4) { .r0  = x  , .yzw = yzw }; }
static inline v4 v4v31  (v3  const xyz, f32 const w  ) { return (v4) { .xyz = xyz, .w   = w   }; }

#define v3xy(XY) ((v3) { .xy = XY })
#define v3yz(YZ) ((v3) { .yz = YZ })
static inline v3 v3xz (v2 const xz) { return V3xz(xz.r0, xz.r1); }

#define v3v(fva, fvb) generic(fva, fop(v3v12), v2: v3v21) (fva, fvb)
static inline v3 v3v12 (f32 const x, v2 const yz) { return (v3) { .r0 = x , .yz = yz }; }
static inline v3 v3v21 (v2 const xy, f32 const z) { return (v3) { .xy = xy, .z  = z  }; }

#define m2m(m) generic(m, m3: m2m3, m4: m2m4) (m)
static inline m2 m2m3(m3 const m) { return (m2) { .c0 = m.c0.xy
                                                , .c1 = m.c1.xy }; }
static inline m2 m2m4(m4 const m) { return (m2) { .c0 = m.c0.xy
                                                , .c1 = m.c1.xy }; }

#define m3m(m) generic(m, m2: m3m2, m4: m3m4) (m)
static inline m3 m3m2(m2 const m) { return (m3) { .c0.xy = m.c0
                                                , .c1.xy = m.c1 }; }
static inline m3 m3m4(m4 const m) { return (m3) { .c0 = m.c0.xyz
                                                , .c1 = m.c1.xyz
                                                , .c2 = m.c2.xyz }; }

#define m4m(m) generic(m, m2: m4m2, m3: m4m3) (m)
static inline m4 m4m2(m2 const m) { return (m4) { .c0.xy = m.c0
                                                , .c1.xy = m.c1 }; }
static inline m4 m4m3(m3 const m) { return (m4) { .c0.xyz = m.c0
                                                , .c1.xyz = m.c1
                                                , .c2.xyz = m.c2 }; }

#define diag(vm) generic(vm, VMop(diag)) (vm)
static inline m2 diagv2(v2 const v) { return M2diag(v.x, v.y); }
static inline m3 diagv3(v3 const v) { return M3diag(v.x, v.y, v.z); }
static inline m4 diagv4(v4 const v) { return M4diag(v.x, v.y, v.z, v.w); }
static inline v2 diagm2(m2 const m) { return (v2) { m.c0.r0
                                                  , m.c1.r1 }; }
static inline v3 diagm3(m3 const m) { return (v3) { m.c0.r0
                                                  , m.c1.r1
                                                  , m.c2.r2 }; }
static inline v4 diagm4(m4 const m) { return (v4) { m.c0.r0
                                                  , m.c1.r1
                                                  , m.c2.r2
                                                  , m.c3.r3 }; }

#define frev(n, pf) generic(pf, pFop(frev)) (n, pf)
static inline void frevf32p(size_t const n, f32 p[static const restrict n])
{
	f32 s;
	for (u32 i = 0; i < n / 2; ++i)
	{ s = p[i]; p[i] = p[n-1-i]; p[n-1-i] = s; }
}
static inline void frevf64p(size_t const n, f64 p[static const restrict n])
{
	f64 s;
	for (u32 i = 0; i < n / 2; ++i)
	{ s = p[i]; p[i] = p[n-1-i]; p[n-1-i] = s; }
}

#define rev(v) generic(v, PVop(rev)) (v)
static inline v2   revv2  (v2 v) { frev(2, v.e); return v; }
static inline v3   revv3  (v3 v) { frev(3, v.e); return v; }
static inline v4   revv4  (v4 v) { frev(4, v.e); return v; }
static inline void revv2p (v2 * const restrict v) { frev(2, v->e); }
static inline void revv3p (v3 * const restrict v) { frev(3, v->e); }
static inline void revv4p (v4 * const restrict v) { frev(4, v->e); }

#define shift(v, o) generic(v, PVop(shift)) (v, o)
static inline v2   shiftv2  (v2 v, u8 o) { o %= 2; frev(2, v.e); frev(o, v.e); frev(2 - o, v.e + o); return v; }
static inline v3   shiftv3  (v3 v, u8 o) { o %= 3; frev(3, v.e); frev(o, v.e); frev(3 - o, v.e + o); return v; }
static inline v4   shiftv4  (v4 v, u8 o) { o %= 4; frev(4, v.e); frev(o, v.e); frev(4 - o, v.e + o); return v; }
static inline void shiftv2p (v2 * const restrict v, u8 o) { o %= 2; frev(2, v->e); frev(o, v->e); frev(2 - o, v->e + o); }
static inline void shiftv3p (v3 * const restrict v, u8 o) { o %= 3; frev(3, v->e); frev(o, v->e); frev(3 - o, v->e + o); }
static inline void shiftv4p (v4 * const restrict v, u8 o) { o %= 4; frev(4, v->e); frev(o, v->e); frev(4 - o, v->e + o); }

#define eq(a, b) generic(a, FVMop(eq), tqtr: eqtr, tqtrs: eqtrs) (a, b)
#define Eq (a.e[e] == b.e[e])
static inline bool eqf32 (f32 const a, f32 const b) { return a == b; }
static inline bool eqf64 (f64 const a, f64 const b) { return a == b; }
static inline bool eqv2 (v2 const a, v2 const b) { for (u8 e = 0; e < 2  ; ++e) if (!Eq) return false; return true; }
static inline bool eqv3 (v3 const a, v3 const b) { for (u8 e = 0; e < 3  ; ++e) if (!Eq) return false; return true; }
static inline bool eqv4 (v4 const a, v4 const b) { for (u8 e = 0; e < 4  ; ++e) if (!Eq) return false; return true; }
static inline bool eqm2 (m2 const a, m2 const b) { for (u8 e = 0; e < 2*2; ++e) if (!Eq) return false; return true; }
static inline bool eqm3 (m3 const a, m3 const b) { for (u8 e = 0; e < 3*3; ++e) if (!Eq) return false; return true; }
static inline bool eqm4 (m4 const a, m4 const b) { for (u8 e = 0; e < 4*4; ++e) if (!Eq) return false; return true; }
static inline bool eqtr  (tqtr  const a, tqtr  const b) { return eqv3(a.t, b.t) && eqv4(a.r, b.r); }
static inline bool eqtrs (tqtrs const a, tqtrs const b) { return eqtr(a.tr, b.tr) && eqf32(a.s, b.s); }
#undef Eq

#define feq(a, b) generic(a, FVMop(feq)) (a, b)
#define Feqf32(a, b) (fabsf(a - b) < FLT_EPSILON)
#define Feqf64(a, b) (fabs (a - b) < DBL_EPSILON)
#define Feq Feqf32(a.e[e], b.e[e])
static inline bool feqf32 (f32 const a, f32 const b) { return Feqf32(a, b); }
static inline bool feqf64 (f64 const a, f64 const b) { return Feqf64(a, b); }
static inline bool feqv2  (v2  const a, v2  const b) { for (u8 e = 0; e < 2  ; ++e) if (!Feq) return false; return true; }
static inline bool feqv3  (v3  const a, v3  const b) { for (u8 e = 0; e < 3  ; ++e) if (!Feq) return false; return true; }
static inline bool feqv4  (v4  const a, v4  const b) { for (u8 e = 0; e < 4  ; ++e) if (!Feq) return false; return true; }
static inline bool feqm2  (m2  const a, m2  const b) { for (u8 e = 0; e < 2*2; ++e) if (!Feq) return false; return true; }
static inline bool feqm3  (m3  const a, m3  const b) { for (u8 e = 0; e < 3*3; ++e) if (!Feq) return false; return true; }
static inline bool feqm4  (m4  const a, m4  const b) { for (u8 e = 0; e < 4*4; ++e) if (!Feq) return false; return true; }
#undef Feq
#undef Feqf64
#undef Feqf32

#define add(a, b) generic(b, fop(generic(a, PVMop(sadd))), default: generic(a, PVMop(add))) (a, b)
static inline v2   saddv2  (v2 a, f32 const s) { for (u8 e = 0; e < 2  ; ++e) a.e[e] += s; return a; }
static inline v3   saddv3  (v3 a, f32 const s) { for (u8 e = 0; e < 3  ; ++e) a.e[e] += s; return a; }
static inline v4   saddv4  (v4 a, f32 const s) { for (u8 e = 0; e < 4  ; ++e) a.e[e] += s; return a; }
static inline m2   saddm2  (m2 a, f32 const s) { for (u8 e = 0; e < 2*2; ++e) a.e[e] += s; return a; }
static inline m3   saddm3  (m3 a, f32 const s) { for (u8 e = 0; e < 3*3; ++e) a.e[e] += s; return a; }
static inline m4   saddm4  (m4 a, f32 const s) { for (u8 e = 0; e < 4*4; ++e) a.e[e] += s; return a; }
static inline void saddv2p (v2 * const restrict a, f32 const s) { for (u8 e = 0; e < 2  ; ++e) a->e[e] += s; }
static inline void saddv3p (v3 * const restrict a, f32 const s) { for (u8 e = 0; e < 3  ; ++e) a->e[e] += s; }
static inline void saddv4p (v4 * const restrict a, f32 const s) { for (u8 e = 0; e < 4  ; ++e) a->e[e] += s; }
static inline void saddm2p (m2 * const restrict a, f32 const s) { for (u8 e = 0; e < 2*2; ++e) a->e[e] += s; }
static inline void saddm3p (m3 * const restrict a, f32 const s) { for (u8 e = 0; e < 3*3; ++e) a->e[e] += s; }
static inline void saddm4p (m4 * const restrict a, f32 const s) { for (u8 e = 0; e < 4*4; ++e) a->e[e] += s; }
static inline v2    addv2  (v2 a, v2 const b) { for (u8 e = 0; e < 2  ; ++e) a.e[e] += b.e[e]; return a; }
static inline v3    addv3  (v3 a, v3 const b) { for (u8 e = 0; e < 3  ; ++e) a.e[e] += b.e[e]; return a; }
static inline v4    addv4  (v4 a, v4 const b) { for (u8 e = 0; e < 4  ; ++e) a.e[e] += b.e[e]; return a; }
static inline m2    addm2  (m2 a, m2 const b) { for (u8 e = 0; e < 2*2; ++e) a.e[e] += b.e[e]; return a; }
static inline m3    addm3  (m3 a, m3 const b) { for (u8 e = 0; e < 3*3; ++e) a.e[e] += b.e[e]; return a; }
static inline m4    addm4  (m4 a, m4 const b) { for (u8 e = 0; e < 4*4; ++e) a.e[e] += b.e[e]; return a; }
static inline void  addv2p (v2 * const restrict a, v2 const b) { for (u8 e = 0; e < 2  ; ++e) a->e[e] += b.e[e]; }
static inline void  addv3p (v3 * const restrict a, v3 const b) { for (u8 e = 0; e < 3  ; ++e) a->e[e] += b.e[e]; }
static inline void  addv4p (v4 * const restrict a, v4 const b) { for (u8 e = 0; e < 4  ; ++e) a->e[e] += b.e[e]; }
static inline void  addm2p (m2 * const restrict a, m2 const b) { for (u8 e = 0; e < 2*2; ++e) a->e[e] += b.e[e]; }
static inline void  addm3p (m3 * const restrict a, m3 const b) { for (u8 e = 0; e < 3*3; ++e) a->e[e] += b.e[e]; }
static inline void  addm4p (m4 * const restrict a, m4 const b) { for (u8 e = 0; e < 4*4; ++e) a->e[e] += b.e[e]; }

#define sub(a, b) generic(b, fop(generic(a, PVMop(ssub))), default: generic(a, PVMop(sub))) (a, b)
static inline v2   ssubv2  (v2 a, const f32 s) { for (u8 e = 0; e < 2  ; ++e) a.e[e] -= s; return a; }
static inline v3   ssubv3  (v3 a, const f32 s) { for (u8 e = 0; e < 3  ; ++e) a.e[e] -= s; return a; }
static inline v4   ssubv4  (v4 a, const f32 s) { for (u8 e = 0; e < 4  ; ++e) a.e[e] -= s; return a; }
static inline m2   ssubm2  (m2 a, const f32 s) { for (u8 e = 0; e < 2*2; ++e) a.e[e] -= s; return a; }
static inline m3   ssubm3  (m3 a, const f32 s) { for (u8 e = 0; e < 3*3; ++e) a.e[e] -= s; return a; }
static inline m4   ssubm4  (m4 a, const f32 s) { for (u8 e = 0; e < 4*4; ++e) a.e[e] -= s; return a; }
static inline void ssubv2p (v2 * const restrict a, const f32 s) { for (u8 e = 0; e < 2  ; ++e) a->e[e] -= s; }
static inline void ssubv3p (v3 * const restrict a, const f32 s) { for (u8 e = 0; e < 3  ; ++e) a->e[e] -= s; }
static inline void ssubv4p (v4 * const restrict a, const f32 s) { for (u8 e = 0; e < 4  ; ++e) a->e[e] -= s; }
static inline void ssubm2p (m2 * const restrict a, const f32 s) { for (u8 e = 0; e < 2*2; ++e) a->e[e] -= s; }
static inline void ssubm3p (m3 * const restrict a, const f32 s) { for (u8 e = 0; e < 3*3; ++e) a->e[e] -= s; }
static inline void ssubm4p (m4 * const restrict a, const f32 s) { for (u8 e = 0; e < 4*4; ++e) a->e[e] -= s; }
static inline v2    subv2  (v2 a, const v2 b) { for (u8 e = 0; e < 2  ; ++e) a.e[e] -= b.e[e]; return a; }
static inline v3    subv3  (v3 a, const v3 b) { for (u8 e = 0; e < 3  ; ++e) a.e[e] -= b.e[e]; return a; }
static inline v4    subv4  (v4 a, const v4 b) { for (u8 e = 0; e < 4  ; ++e) a.e[e] -= b.e[e]; return a; }
static inline m2    subm2  (m2 a, const m2 b) { for (u8 e = 0; e < 2*2; ++e) a.e[e] -= b.e[e]; return a; }
static inline m3    subm3  (m3 a, const m3 b) { for (u8 e = 0; e < 3*3; ++e) a.e[e] -= b.e[e]; return a; }
static inline m4    subm4  (m4 a, const m4 b) { for (u8 e = 0; e < 4*4; ++e) a.e[e] -= b.e[e]; return a; }
static inline void  subv2p (v2 * const restrict a, const v2 b) { for (u8 e = 0; e < 2  ; ++e) a->e[e] -= b.e[e]; }
static inline void  subv3p (v3 * const restrict a, const v3 b) { for (u8 e = 0; e < 3  ; ++e) a->e[e] -= b.e[e]; }
static inline void  subv4p (v4 * const restrict a, const v4 b) { for (u8 e = 0; e < 4  ; ++e) a->e[e] -= b.e[e]; }
static inline void  subm2p (m2 * const restrict a, const m2 b) { for (u8 e = 0; e < 2*2; ++e) a->e[e] -= b.e[e]; }
static inline void  subm3p (m3 * const restrict a, const m3 b) { for (u8 e = 0; e < 3*3; ++e) a->e[e] -= b.e[e]; }
static inline void  subm4p (m4 * const restrict a, const m4 b) { for (u8 e = 0; e < 4*4; ++e) a->e[e] -= b.e[e]; }

#define sum(v) generic(v, Vop(sum)) (v)
static inline f32 sumv2 (const v2 v) { return v.x + v.y; };
static inline f32 sumv3 (const v3 v) { return v.x + v.y + v.z; };
static inline f32 sumv4 (const v4 v) { return v.x + v.y + v.z + v.w; };

#define schur(a, b) generic(a, PVMop(schur)) (a, b)
static inline v2   schurv2  (v2 a, const v2 b) { for (u8 e = 0; e < 2  ; ++e) a.e[e] *= b.e[e]; return a; }
static inline v3   schurv3  (v3 a, const v3 b) { for (u8 e = 0; e < 3  ; ++e) a.e[e] *= b.e[e]; return a; }
static inline v4   schurv4  (v4 a, const v4 b) { for (u8 e = 0; e < 4  ; ++e) a.e[e] *= b.e[e]; return a; }
static inline m2   schurm2  (m2 a, const m2 b) { for (u8 e = 0; e < 2*2; ++e) a.e[e] *= b.e[e]; return a; }
static inline m3   schurm3  (m3 a, const m3 b) { for (u8 e = 0; e < 3*3; ++e) a.e[e] *= b.e[e]; return a; }
static inline m4   schurm4  (m4 a, const m4 b) { for (u8 e = 0; e < 4*4; ++e) a.e[e] *= b.e[e]; return a; }
static inline void schurv2p (v2 * const restrict a, const v2 b) { for (u8 e = 0; e < 2  ; ++e) a->e[e] *= b.e[e]; }
static inline void schurv3p (v3 * const restrict a, const v3 b) { for (u8 e = 0; e < 3  ; ++e) a->e[e] *= b.e[e]; }
static inline void schurv4p (v4 * const restrict a, const v4 b) { for (u8 e = 0; e < 4  ; ++e) a->e[e] *= b.e[e]; }
static inline void schurm2p (m2 * const restrict a, const m2 b) { for (u8 e = 0; e < 2*2; ++e) a->e[e] *= b.e[e]; }
static inline void schurm3p (m3 * const restrict a, const m3 b) { for (u8 e = 0; e < 3*3; ++e) a->e[e] *= b.e[e]; }
static inline void schurm4p (m4 * const restrict a, const m4 b) { for (u8 e = 0; e < 4*4; ++e) a->e[e] *= b.e[e]; }

#define magsq(v) generic(v, Vop(magsq)) (v)
static inline f32 magsqv2 (v2 const v) { f32 s = 0.f; for (u8 e = 0; e < 2; ++e) s += v.e[e] * v.e[e]; return s; }
intrin(inline f32 magsqv3 (v3 const v) { f32 s = 0.f; for (u8 e = 0; e < 3; ++e) s += v.e[e] * v.e[e]; return s; })
intrin(inline f32 magsqv4 (v4 const v) { f32 s = 0.f; for (u8 e = 0; e < 4; ++e) s += v.e[e] * v.e[e]; return s; })

#define dot(a, b) generic(a, Vop(dot)) (a, b)
static inline f32 dotv2 (const v2 a, const v2 b) { f32 s = 0.f; for (u8 e = 0; e < 2; ++e) s += a.e[e] * b.e[e]; return s; }
intrin(inline f32 dotv3 (const v3 a, const v3 b) { f32 s = 0.f; for (u8 e = 0; e < 3; ++e) s += a.e[e] * b.e[e]; return s; })
intrin(inline f32 dotv4 (const v4 a, const v4 b) { f32 s = 0.f; for (u8 e = 0; e < 4; ++e) s += a.e[e] * b.e[e]; return s; })

static inline v3 cross(v3 const a, v3 const b) { return (v3) { a.y*b.z - a.z*b.y
                                                             , a.z*b.x - a.x*b.z
                                                             , a.x*b.y - a.y*b.x }; }

#define Neg(a) ((a) = -(a))

#define neg(a) generic(a, PVMop(neg), PFop(neg)) (a)
static inline v2   negv2   (v2 a) { for (u8 e = 0; e < 2  ; ++e) Neg(a.e[e]); return a; }
static inline v3   negv3   (v3 a) { for (u8 e = 0; e < 3  ; ++e) Neg(a.e[e]); return a; }
static inline v4   negv4   (v4 a) { for (u8 e = 0; e < 4  ; ++e) Neg(a.e[e]); return a; }
static inline m2   negm2   (m2 a) { for (u8 e = 0; e < 2*2; ++e) Neg(a.e[e]); return a; }
static inline m3   negm3   (m3 a) { for (u8 e = 0; e < 3*3; ++e) Neg(a.e[e]); return a; }
static inline m4   negm4   (m4 a) { for (u8 e = 0; e < 4*4; ++e) Neg(a.e[e]); return a; }
static inline void negv2p  (v2 * const restrict a) { for (u8 e = 0; e < 2  ; ++e) Neg(a->e[e]); }
static inline void negv3p  (v3 * const restrict a) { for (u8 e = 0; e < 3  ; ++e) Neg(a->e[e]); }
static inline void negv4p  (v4 * const restrict a) { for (u8 e = 0; e < 4  ; ++e) Neg(a->e[e]); }
static inline void negm2p  (m2 * const restrict a) { for (u8 e = 0; e < 2*2; ++e) Neg(a->e[e]); }
static inline void negm3p  (m3 * const restrict a) { for (u8 e = 0; e < 3*3; ++e) Neg(a->e[e]); }
static inline void negm4p  (m4 * const restrict a) { for (u8 e = 0; e < 4*4; ++e) Neg(a->e[e]); }
static inline f32  negf32  (f32 const s) { return -s; }
static inline f64  negf64  (f64 const s) { return -s; }
static inline void negf32p (f32 * const restrict s) { Neg(*s); }
static inline void negf64p (f64 * const restrict s) { Neg(*s); }

// would be truly cursed to try to enable mul(s, v) and mul(s, m) as well lol
#define mul(a, b) generic(b, fop(generic(a, PVMop(mul))) \
                           , v4: generic(a, v3: vmulq, v4: qmulq, default: NULL) \
                           , Mop(mmul) ) (a, b)
static inline v2   mulv2  (v2 const v, f32 const s) { return V2smul(s, v.x, v.y); }
static inline v3   mulv3  (v3 const v, f32 const s) { return V3smul(s, v.x, v.y, v.z); }
static inline v4   mulv4  (v4 const v, f32 const s) { return V4smul(s, v.x, v.y, v.z, v.w); }
static inline m2   mulm2  (m2 const m, f32 const s) { return M2smul(s, m.c0.r0, m.c0.r1
                                                                     , m.c1.r0, m.c1.r1 ); }
static inline m3   mulm3  (m3 const m, f32 const s) { return M3smul(s, m.c0.r0, m.c0.r1, m.c0.r2
                                                                     , m.c1.r0, m.c1.r1, m.c1.r2
                                                                     , m.c2.r0, m.c2.r1, m.c2.r2 ); }
static inline m4   mulm4  (m4 const m, f32 const s) { return M4smul(s, m.c0.r0, m.c0.r1, m.c0.r2, m.c0.r3
                                                                     , m.c1.r0, m.c1.r1, m.c1.r2, m.c1.r3
                                                                     , m.c2.r0, m.c2.r1, m.c2.r2, m.c2.r3
                                                                     , m.c3.r0, m.c3.r1, m.c3.r2, m.c3.r3 ); }
static inline void mulv2p (v2 * const restrict v, f32 const s) { for (u8 e = 0; e < 2  ; ++e) v->e[e] *= s; }
static inline void mulv3p (v3 * const restrict v, f32 const s) { for (u8 e = 0; e < 3  ; ++e) v->e[e] *= s; }
static inline void mulv4p (v4 * const restrict v, f32 const s) { for (u8 e = 0; e < 4  ; ++e) v->e[e] *= s; }
static inline void mulm2p (m2 * const restrict m, f32 const s) { for (u8 e = 0; e < 2*2; ++e) m->e[e] *= s; }
static inline void mulm3p (m3 * const restrict m, f32 const s) { for (u8 e = 0; e < 3*3; ++e) m->e[e] *= s; }
static inline void mulm4p (m4 * const restrict m, f32 const s) { for (u8 e = 0; e < 4*4; ++e) m->e[e] *= s; }
// remember, SLOW!!! NOT OPTIMIZED!!! JUST FOR FUN!!!
#define mmulm(N) \
static inline m##N mmulm##N(m##N const a, m##N const b) { m##N m = { 0 }; \
                                                          for (u8 c = 0; c < N; ++c) \
                                                          for (u8 x = 0; x < N; ++x) \
                                                          for (u8 r = 0; r < N; ++r) \
                                                          m.c[c].r[r] += b.c[c].r[x] * a.c[x].r[r]; \
                                                          return m; }
mmulm(2)
mmulm(3)
mmulm(4)
#undef mmulm

static inline v4 qmulq(v4 a, v4 const b) // Hamilton convention
{
	v4 q;
	Neg(a.z); // {  x,  y, -z,  w }
	q.x = dot(a, vwzyx(b));
	Neg(a.z),
	Neg(a.x); // { -x,  y,  z,  w }
	q.y = dot(a, vzwxy(b));
	Neg(a.x),
	Neg(a.y); // {  x, -y,  z,  w }
	q.z = dot(a, vyxwz(b));
	Neg(a.x),
	Neg(a.z); // { -x, -y, -z,  w } (qconj)
	q.w = dot(a,       b );
	return q;
}

// https://gamedev.stackexchange.com/questions/28395/rotating-vector3-by-a-quaternion
static inline v3 vmulq(const v3 v, const v4 q)
{
	v3 r =  mul(q.xyz          , 2.f * dot(q.xyz, v)     );
	add(&r, mul(v              , q.w * q.w - magsq(q.xyz)));
	add(&r, mul(cross(q.xyz, v), 2.f * q.w               ));
	return r;
}

#define print(...) fprint(stdout, __VA_ARGS__)

#define fprint(stream, a) generic(a, f32: fprintf32, VMop(fprint)) (stream, a)
#define                                                                   FPRINTF32(stream, f32, c) fprintf(stream, "%8.4f%c", f32, c)
static inline void fprintf32(FILE * const restrict stream, const f32 s) { FPRINTF32(stream, s, '\n'); }
static inline void fprintv2 (FILE * const restrict stream, const v2  v) { for (u8 e = 0; e < 2  ; ++e)
                                                                          FPRINTF32(stream, v.e[e], (e + 1) % 2 ? ',' : '\n'); }
static inline void fprintv3 (FILE * const restrict stream, const v3  v) { for (u8 e = 0; e < 3  ; ++e)
                                                                          FPRINTF32(stream, v.e[e], (e + 1) % 3 ? ',' : '\n'); }
static inline void fprintv4 (FILE * const restrict stream, const v4  v) { for (u8 e = 0; e < 4  ; ++e)
                                                                          FPRINTF32(stream, v.e[e], (e + 1) % 4 ? ',' : '\n'); }
static inline void fprintm2 (FILE * const restrict stream, const m2  m) { for (u8 e = 0; e < 2*2; ++e)
                                                                          FPRINTF32(stream, m.c[e%2].r[e/2], (e + 1) % 2 ? ',' : '\n'); }
static inline void fprintm3 (FILE * const restrict stream, const m3  m) { for (u8 e = 0; e < 3*3; ++e)
                                                                          FPRINTF32(stream, m.c[e%3].r[e/3], (e + 1) % 3 ? ',' : '\n'); }
static inline void fprintm4 (FILE * const restrict stream, const m4  m) { for (u8 e = 0; e < 4*4; ++e)
                                                                          FPRINTF32(stream, m.c[e%4].r[e/4], (e + 1) % 4 ? ',' : '\n'); }
#undef                                                                    FPRINTF32

#define mag(v) sqrt(magsq(v))
#define isnorm(v) (fabs(1.f - magsq(v)) < 1e-6) // why 1e-6 and not FLT_EPSILON (1e-5)

#define norm(v) generic(v, PVop(norm)) (v)
static inline v2   normv2  (v2 v) { f32 s = 1.f / mag(v); for (u8 e = 0; e < 2; ++e) v.e[e] *= s; return v; }
static inline v3   normv3  (v3 v) { f32 s = 1.f / mag(v); for (u8 e = 0; e < 3; ++e) v.e[e] *= s; return v; }
static inline v4   normv4  (v4 v) { f32 s = 1.f / mag(v); for (u8 e = 0; e < 4; ++e) v.e[e] *= s; return v; }
static inline void normv2p (v2 * const restrict v) { f32 s = 1.f / mag(*v); for (u8 e = 0; e < 2; ++e) v->e[e] *= s; }
static inline void normv3p (v3 * const restrict v) { f32 s = 1.f / mag(*v); for (u8 e = 0; e < 3; ++e) v->e[e] *= s; }
static inline void normv4p (v4 * const restrict v) { f32 s = 1.f / mag(*v); for (u8 e = 0; e < 4; ++e) v->e[e] *= s; }

#define row(m) generic(m, PMop(row)) (m)
static inline v2 rowm2  (m2 const m, const u8 r) { v2 v; for (u8 c = 0; c < 2; ++c) v.e[c] = m.c[c].r[r]; return v; }
static inline v3 rowm3  (m3 const m, const u8 r) { v3 v; for (u8 c = 0; c < 3; ++c) v.e[c] = m.c[c].r[r]; return v; }
static inline v4 rowm4  (m4 const m, const u8 r) { v4 v; for (u8 c = 0; c < 4; ++c) v.e[c] = m.c[c].r[r]; return v; }
static inline v2 rowm2p (m2 const * const restrict m, const u8 r) { v2 v; for (u8 c = 0; c < 2; ++c) v.e[c] = m->c[c].r[r]; return v; }
static inline v3 rowm3p (m3 const * const restrict m, const u8 r) { v3 v; for (u8 c = 0; c < 3; ++c) v.e[c] = m->c[c].r[r]; return v; }
static inline v4 rowm4p (m4 const * const restrict m, const u8 r) { v4 v; for (u8 c = 0; c < 4; ++c) v.e[c] = m->c[c].r[r]; return v; }

#define trace(m) generic(m, PMop(trace)) (m)
static inline f32 tracem2  (m2 const m) { return sum(diag(m)); };
static inline f32 tracem3  (m3 const m) { return sum(diag(m)); };
static inline f32 tracem4  (m4 const m) { return sum(diag(m)); };
static inline f32 tracem2p (m2 const * const restrict m) { f32 s = 0; for (u8 d = 0; d < 2; ++d) s += m->c[d].r[d]; return s; }
static inline f32 tracem3p (m3 const * const restrict m) { f32 s = 0; for (u8 d = 0; d < 3; ++d) s += m->c[d].r[d]; return s; }
static inline f32 tracem4p (m4 const * const restrict m) { f32 s = 0; for (u8 d = 0; d < 4; ++d) s += m->c[d].r[d]; return s; }

#define fsign(s) generic(s, Fop(fsign)) (s)
static inline f32 fsignf32(const f32 s) { return (f32) (*(u32 *) &s >> 0x1F) * -2.f + 1.f; }
static inline f64 fsignf64(const f64 s) { return (f64) (*(u64 *) &s >> 0x3F) * -2.  + 1. ; }

#define fclamp(a, ...) generic(a, f32: fclampf32, v2: fclampv2) (a, __VA_ARGS__)
static inline f32 fclampf32(const f32 min
                           ,const f32 max
                           ,const f32 s) { return fmin(max, fmax(min, s)); }
static inline f32 fclampv2 (const v2  v
                           ,const f32 s) { return fclampf32(v.x, v.y, s); }

#define flerp(a, ...) generic(a, f32: flerpf32, v2: flerpv2) (a, __VA_ARGS__)
static inline f32 flerpf32(const f32 a
                          ,const f32 b
                          ,const f32 t) { return (1.f - t) * a + t * b; }
static inline f32 flerpv2 (const v2  v
                          ,const f32 t) { return flerpf32(v.x, v.y, t); }

static inline f32 fclamp01(const f32 s) { return fclampf32(0.f, 1.f, s); }

#define flerp01(a, ...) generic(a, f32: flerp01f32, v2: flerp01v2) (a, __VA_ARGS__)
static inline f32 flerp01f32(const f32 a
                            ,const f32 b
                            ,const f32 t) { return flerp(a, b, fclamp01(t)); }
static inline f32 flerp01v2 (const v2  v
                            ,const f32 t) { return flerp01f32(v.x, v.y, t); }

#define lerp(a, b, t) add(mul(a, (1.f - t)), mul(b, t))
#define lerp01(a, b, t) lerp(a, b, fclamp01(t))

static inline v4 qAxisAngle(const v3 axis, const f32 angle)
{
	algassert(isnorm(axis));
	const f32 half = .5f * angle;
	return v4v(mul(axis, sin(half)), cos(half));
}

static inline v4 nlerp(const v4 a, const v4 b, const f32 t)
	{ return norm(add(a, mul(sub(b, a), t))); }

#define qconj(q) generic(q, v4: qconjv4, v4*: qconjv4p) (q)
static inline v4   qconjv4  (v4 const q)
	{ return V4(-q.x, -q.y, -q.z, q.w); }
static inline void qconjv4p (v4 * const restrict q)
	{ neg(&q->xyz); }

static inline v4 rm3(const m3 r)
{
	// TODO debug check special orthogonal
	const f32 rt = trace(r);
	if (rt > 0.f) {
		const f32 s = 2.f * sqrt(1.f + rt);
		return V4( (r.c1.r2 - r.c2.r1) / s
		         , (r.c2.r0 - r.c0.r2) / s
		         , (r.c0.r1 - r.c1.r0) / s
		         ,                .25f * s );
	} else if ((r.c0.r0 > r.c1.r1) && (r.c0.r0 > r.c2.r2)) {
		const f32 s = 2.f * sqrt(1.f + r.c0.r0 - r.c1.r1 - r.c2.r2);
		return V4(                .25f * s
		         , (r.c1.r0 + r.c0.r1) / s
		         , (r.c2.r0 + r.c0.r2) / s
		         , (r.c1.r2 - r.c2.r1) / s );
	} else if (r.c1.r1 > r.c2.r2) {
		const f32 s = 2.f * sqrt(1.f - r.c0.r0 + r.c1.r1 - r.c2.r2);
		return V4( (r.c1.r0 + r.c0.r1) / s
		         ,                .25f * s
		         , (r.c2.r1 + r.c1.r2) / s
		         , (r.c2.r0 - r.c0.r2) / s );
	} else {
		const f32 s = 2.f * sqrt(1.f - r.c0.r0 - r.c1.r1 + r.c2.r2);
		return V4( (r.c2.r0 + r.c0.r2) / s
		         , (r.c2.r1 + r.c1.r2) / s
		         ,                .25f * s
		         , (r.c0.r1 - r.c1.r0) / s );
	}
}

static inline v4 qfwd(const v3 fwd, const v3 up) // assumes fwd and up are not orthogonal
{
	algassert(isnorm(fwd));
	v3 rt = norm(cross(up, fwd));
	return rm3(M3c(rt, cross(fwd, rt), fwd));
}

static inline v4 qup(const v3 fwd, const v3 up) // assumes fwd and up are not orthogonal
{
	algassert(isnorm(up));
	v3 rt = norm(cross(up, fwd));
	return rm3(M3c(rt, up, cross(rt, up)));
}

static inline v4 qrt(const v3 fwd, const v3 up)
	{ return rm3(M3c(cross(up, fwd), up, fwd)); }

static inline m4 m4t(const v3 t)
	{ m4 m = M4i; m.c3.xyz = t; return m; }

static inline m3 m3r(const v4 r) // Hamilton convention
{
	const f32 xx = r.x * r.x
	,         xy = r.x * r.y
	,         xz = r.x * r.z
	,         xw = r.x * r.w
	,         yy = r.y * r.y
	,         yz = r.y * r.z
	,         yw = r.y * r.w
	,         zz = r.z * r.z
	,         zw = r.z * r.w
	,         ww = r.w * r.w
	,         s  = 1.f / (+ xx + yy + zz + ww)
	,         x  = s   * (+ xx - yy - zz + ww)
	,         y  = s   * (- xx + yy - zz + ww)
	,         z  = s   * (- xx - yy + zz + ww)
	,         s2 = s   * 2.f ;

	return (m3) { x             , s2 * (xy + zw), s2 * (xz - yw)
	            , s2 * (xy - zw), y             , s2 * (yz + xw)
	            , s2 * (xz + yw), s2 * (yz - xw), z              };
}

static inline m4 m4r(const v4 r)
{
	m4 m = m4m(m3r(r));
	m.c3.w = 1.f;
	return m;
}

#define m3s(s) generic(s, f32: m3sf32, v3: m3sv3) (s)
static inline m3 m3sf32 (const f32 s) { return diag(V3e(s)); }
static inline m3 m3sv3  (const v3  s) { return diag(    s ); }

#define m4s(s) generic(s, f32: m4sf32, v3: m4sv3) (s)
static inline m4 m4sf32 (const f32 s) { return diag(V4ew(s, 1.f)); }
static inline m4 m4sv3  (const v3  s) { return diag( v4v(s, 1.f)); }

#define _m4tr(a, ...) generic(a, tqtr: m4trtqtr, v3: m4trv3)
#define m4tr(...) _m4tr(__VA_ARGS__) (__VA_ARGS__)
static inline m4 m4trv3(v3 const t, v4 const r)
	{ m4 m = m4r(r); m.c3 = v4v(t, 1.f); return m; }
static inline m4 m4trtqtr(tqtr const tr)
	{ return m4trv3(tr.t, tr.r); }

#define _m4view(a, ...) generic(a, tqtr: m4viewtqtr, v3: m4viewv3)
#define m4view(...) _m4view(__VA_ARGS__) (__VA_ARGS__)
static inline m4 m4viewv3(const v3 t, const v4 r)
	{ return m4tr(neg(t), qconj(r)); }
static inline m4 m4viewtqtr(const tqtr tr)
	{ return m4viewv3(tr.t, tr.r); }

#define _m4trs(a, ...) generic(a, v3: m4trsv3, tqtrs: m4trstqtrs)
#define m4trs(...) _m4trs(__VA_ARGS__) (__VA_ARGS__)
static inline m4 m4trsv3(v3 const t, v4 const r, f32 const s)
	{ m4 m = m4m(mul(m3r(r), s)); m.c3 = v4v(t, 1.f); return m; }
static inline m4 m4trstqtrs(tqtrs trs)
	{ return m4trsv3(trs.t, trs.r, trs.s); }

static inline m4 m4trsv(v3 const t, v4 const r, v3 const s)
	{ m4 m = m4m(mul(m3r(r), m3s(s))); m.c3 = v4v(t, 1.f); return m; }

static inline m4 _projection
	( f32  const yScale
	, f32  const xyAspectRatio
	, f32  const zNear
	, f32  const zFar
	, bool const yInverted )
{
	const f32 zRange = zFar - zNear;
	return (m4)
		{ .c0.x =  yScale / xyAspectRatio       // x scale
		, .c1.y =  yInverted ? -yScale : yScale // y scale
		, .c2.z =  1.f / zRange                 // z scale
		, .c3.z = -zNear / zRange };            // z translate
}

#define m4persp m4perspective
static inline m4 m4perspective
	( f32  const vfov // vertical field of view in degrees
	, f32  const xyAspectRatio
	, f32  const zNear
	, f32  const zFar
	, bool const yInverted )
{
	m4 projection = _projection
		( 1.f / tan((vfov / 180.f) * .5 * AlgPi)
		, xyAspectRatio
		, zNear
		, zFar
		, yInverted );
	projection.c2.w = 1.f; // perspective projection
	return projection;
}

#define m4ortho m4orthographic
static inline m4 m4orthographic
	( f32  const height // number of units that fit in vertical height
	, f32  const xyAspectRatio
	, f32  const zNear
	, f32  const zFar
	, bool const yInverted )
{
	m4 projection = _projection
		( 2.f / height
		, xyAspectRatio
		, zNear
		, zFar
		, yInverted );
	projection.c3.w = 1.f; // orthographic projection
	return projection;
}

static inline m4 m4screen
	( f32  const width
	, f32  const height
	, bool const yInverted )
{
	return (m4) // min(0,0,0) max(width,height,1)
		{ .c0.x =  2.f / width
		, .c1.y = (yInverted ? -2.f :  2.f) / height
		, .c2.z =  1.f
		, .c3.x = -1.f
		, .c3.y =  yInverted ?  1.f : -1.f
		, .c3.w =  1.f };
}

#ifdef AlgInline
# undef inline
#endif

#ifdef AlgTest
# include "algtest.h"
#else
# define algtest(...)
#endif
